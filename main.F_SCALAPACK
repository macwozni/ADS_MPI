      program main
c    
      use parallelism
      use projection_engine
      use communicators
c
      implicit none
c
      include "mpif.h"
c
c-> SCALAPACK GRID
      
c<-
      integer(kind=4) :: n, p, nelem
      real   (kind=8), allocatable, dimension(:) :: U
      double precision, allocatable, dimension(:,:) :: M
c-> DEBUG
      double precision, allocatable, dimension(:,:) :: MFULL
      double precision, allocatable, dimension(:) :: FFULL
c<-DEBUG
      double precision, allocatable, dimension(:,:) :: F
      double precision, allocatable, dimension(:,:) :: F2
      double precision, allocatable, dimension(:,:) :: F3
      integer :: KL, KU
      integer :: nrcppx,nrcppy,nrcppz
      integer :: ibegx,iendx,ibegy,iendy,ibegz,iendz
      integer :: i
      integer :: iret,ierr
      integer :: idebug,iprint
c-> call to DESCINIT
      integer,allocatable,dimension(:) :: DESCA
      integer,allocatable,dimension(:) :: DESCB
      integer :: MM,NM,MB,NB,INFO
      integer :: IRSRC,ICSRC,MXLLDA
c<-
c-> call to PDDBTRS
      character :: TRANS
      integer :: Nhere
      integer :: BWL, BWU
      integer :: NRHS
      integer :: JA,IB      
      double precision, allocatable, dimension(:) :: AF
      integer :: LAF
      double precision, allocatable, dimension(:) :: WORK
      integer :: LWORK
c<-
      idebug=0
      iprint=0
c      if(MYRANK.eq.0)iprint=1
c
      call initialize_parameters
      call initialize_parallelism
      call create_communicators
      call create_grids
c
c      if(MYRANK.eq.1)iprint=1
c
c  ...prepare the problem dimensions
      p=ORDER !order
      n=SIZE !intervals
c
      if(SIZE<NRPROCX.or.SIZE<NRPROCY.or.SIZE<NRPROCZ)then
        write(*,*)'Number of elements smaller than number of processors'
        stop
      endif
c
      allocate(U(n+p+2)) !knot vector
      KL=p; KU=p
c PARALLEL - distributed
c number of columns per processors
      nrcppx = (n+1+1)/NRPROCX
      if(MYRANKX.eq.NRPROCX-1)then
        ibegx = nrcppx*(NRPROCX-1)+1
        iendx = n+1
      else
        ibegx = nrcppx*(NRPROCX-2)+1
        iendx = nrcppx*(NRPROCX-1)
      endif
      nrcppy = (n+1+1)/NRPROCY
      if(MYRANKY.eq.NRPROCY-1)then
        ibegy = nrcppy*(NRPROCY-1)+1
        iendy = n+1
      else
        ibegy = nrcppy*(NRPROCY-2)+1
        iendy = nrcppy*(NRPROCY-1)
      endif
      nrcppz = (n+1+1)/NRPROCZ
      if(MYRANKZ.eq.NRPROCZ-1)then
        ibegz = nrcppz*(NRPROCZ-1)+1
        iendz = n+1
      else
        ibegz = nrcppz*(NRPROCZ-2)+1
        iendz = nrcppz*(NRPROCZ-1)
      endif
      if(iprint.eq.1)then
        write(*,*)PRINTRANK,
     .     'MYRANKX,MYRANKY,MYRANKZ',MYRANKX,MYRANKY,MYRANKZ
        write(*,*)PRINTRANK,
     .    'NRPROCX,NRPROCY,NRPROCZ',NRPROCX,NRPROCY,NRPROCZ
        write(*,*)PRINTRANK,'n+1',n+1
        write(*,*)PRINTRANK,'nrcppx,nrcppy,nrcppz',nrcppx,nrcppy,nrcppz
        write(*,*)PRINTRANK,'ibegx,iendx',ibegx,iendx
        write(*,*)PRINTRANK,'ibegy,iendy',ibegy,iendy
        write(*,*)PRINTRANK,'ibegz,iendz',ibegz,iendz
      endif
c Now, we distribute RHS matrices

      allocate(M(2*KL+KU+1,iendx-ibegx+1))
      allocate( F((iendx-ibegx+1),(iendy-ibegy+1)*(iendz-ibegz+1))) !x,y,z
      allocate(F2((iendy-ibegy+1),(iendx-ibegx+1)*(iendz-ibegz+1))) !y,x,z
      allocate(F3((iendz-ibegz+1),(iendx-ibegx+1)*(iendy-ibegy+1))) !z,x,y
c      allocate(M(2*KL+KU+1,n+1))
c      allocate(F((n+1),(n+1)*(n+1))) !x,y,z
c      allocate(F2((n+1),(n+1)*(n+1))) !y,x,z
c      allocate(F3((n+1),(n+1)*(n+1))) !z,x,y
c-> DEBUG
      if(idebug.eq.1)then
        allocate(MFULL(n+1,n+1))
        allocate(FFULL((n+1)*(n+1)*(n+1)))
      endif
c<- DEBUG
c .....CREATE DESCRIPTOR ARRAY FOR MATRIX M
c
*  DESCA   (output) INTEGER array of dimension DLEN_.
c          is a descriptor array for matrix M
      allocate(DESCA(9))
*  MM     (global input) INTEGER
*          The number of rows in the distributed matrix M
      MM=2*KL+KU+1
*  NM      (global input) INTEGER
*          The number of columns in the distributed matrix M
      NM=n+1
*  MB      (global input) INTEGER
*          The blocking factor used to distribute the rows of M
      MB= 2*KL+KU+1
*  NB      (global input) INTEGER
*          The blocking factor used to distribute the columns of M
      NB=(iendx-ibegx+1)
*  IRSRC   (global input) INTEGER
*          The process row over which the first row of the matrix is
*          distributed. 0 <= IRSRC < NPROW.
      IRSRC=0 !processorsX(1)
*  ICSRC   (global input) INTEGER
*          The process column over which the first column of the
*          matrix is distributed. 0 <= ICSRC < NPCOL.
      ICSRC=IRSRC
*  CONTXTX (global input) INTEGER
*          The BLACS context handle, indicating the global context of
*          the operation on the matrix. The context itself is global.
*  MXLLDA  (local input)  INTEGER
*          The leading dimension of the local array storing the local
*          blocks of the distributed matrix. LLD >= MAX(1,LOCp(M)).
      MXLLDA=2*KL+KU+1
*  INFO    (output) INTEGER
*          = 0: successful exit
*          < 0: if INFO = -i, the i-th argument had an illegal value
      if(iprint.eq.1)then
        write(*,*)PRINTRANK,'CALL DESCINIT for DESCA for M with'
        write(*,*)PRINTRANK,'MM',MM
        write(*,*)PRINTRANK,'NM',NM
        write(*,*)PRINTRANK,'MB',MB
        write(*,*)PRINTRANK,'NB',NB
        write(*,*)PRINTRANK,'IRSRC',IRSRC
        write(*,*)PRINTRANK,'ICSRC',ICSRC
        write(*,*)PRINTRANK,'CONTEXT',CONTEXTX
        write(*,*)PRINTRANK,'MXLLDA',MXLLDA
      endif

      CALL DESCINIT( DESCA, MM, NM, MB, NB, IRSRC, ICSRC, CONTEXTX, 
     $               MXLLDA,
     $               INFO )
      if(INFO.ne.0)then
        write(*,*)'DESCINIT for DESCA returned',INFO
        stop
      endif

c      if(iprint.eq.1)then
        write(*,*)PRINTRANK,'CALL DESCINIT returned'
        write(*,*)PRINTRANK,'DESCA',DESCA
        write(*,*)PRINTRANK,'INFO',INFO
c      endif

      call mpi_barrier(MPI_COMM_WORLD,ierr)

c .....CREATE DESCRIPTOR ARRAY FOR MATRIX F (RHS)
c
*  DESCA   (output) INTEGER array of dimension DLEN_.
c          is a descriptor array for matrix M
      allocate(DESCB(9))
*  MM     (global input) INTEGER
*          The number of rows in the distributed matrix M
      MM=n+1
*  NM      (global input) INTEGER
*          The number of columns in the distributed matrix M
      NM=(n+1)*(n+1)
*  MB      (global input) INTEGER
*          The blocking factor used to distribute the rows of M
      MB= n+1 !(iendx-ibegx+1)
*  NB      (global input) INTEGER
*          The blocking factor used to distribute the columns of M
      NB= (iendy-ibegy+1)*(iendz-ibegz+1)
*  IRSRC   (global input) INTEGER
*          The process row over which the first row of the matrix is
*          distributed. 0 <= IRSRC < NPROW.
      IRSRC=0 !processorsX(1)
*  ICSRC   (global input) INTEGER
*          The process column over which the first column of the
*          matrix is distributed. 0 <= ICSRC < NPCOL.
      ICSRC=IRSRC
*  CONTXTX (global input) INTEGER
*          The BLACS context handle, indicating the global context of
*          the operation on the matrix. The context itself is global.
*  MXLLDA  (local input)  INTEGER
*          The leading dimension of the local array storing the local
*          blocks of the distributed matrix. LLD >= MAX(1,LOCp(M)).
      MXLLDA=n+1 !(iendx-ibegx+1)
*  INFO    (output) INTEGER
*          = 0: successful exit
*          < 0: if INFO = -i, the i-th argument had an illegal value

      if(iprint.eq.1)then
        write(*,*)PRINTRANK,'CALL DESCINIT for DESCB for F with'
        write(*,*)PRINTRANK,'MM',MM
        write(*,*)PRINTRANK,'NM',NM
        write(*,*)PRINTRANK,'MB',MB
        write(*,*)PRINTRANK,'NB',NB
        write(*,*)PRINTRANK,'IRSRC',IRSRC
        write(*,*)PRINTRANK,'ICSRC',ICSRC
        write(*,*)PRINTRANK,'CONTEXT',CONTEXTX
        write(*,*)PRINTRANK,'MXLLDA',MXLLDA
        write(*,*)PRINTRANK,'returned'
        write(*,*)PRINTRANK,'DESCA',DESCB
        write(*,*)PRINTRANK,'INFO',INFO
      endif

      CALL DESCINIT( DESCB, MM, NM, MB, NB, IRSRC, ICSRC, CONTEXTX, 
     $               MXLLDA,
     $               INFO )
      if(INFO.ne.0)then
        write(*,*)'DESCINIT for DESCB returned',INFO
        stop
      endif

      if(iprint.eq.1)then
        write(*,*)PRINTRANK,'CALL DESCINIT returned'
        write(*,*)PRINTRANK,'DESCA',DESCB
        write(*,*)PRINTRANK,'INFO',INFO
      endif
cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc
c
      U(1:p+1)=0.d0
      U(n+2:n+p+2)=1.d0
      do i=p+2,n+1
c        if(iprint.eq.1)then
c          write(*,*)'i-p-1,n,(i-p-1)/n',i-p-1,n,real(i-p-1)/real(n)
c        endif
        U(i)=real(i-p-1)/real(n)
      enddo
c
      nelem = CountSpans(n,p,U)
      if(iprint.eq.1)then
        write(*,*)'n,p,nelem',n,p,nelem
        write(*,*)'U',U
      endif
c
c  ...generate the 1D matrix
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)

      M=0
      call Form1DMassMatrix(KL,KU,U,p,n,nelem,
     .   ibegx,iendx,MYRANKX,NRPROCX,M) 
      if(iprint.eq.1)then
        write(*,*)PRINTRANK,'M'
        do i=1,2*KL+KU+1
          write(*,*)PRINTRANK,M(i,1:iendx-ibegx+1)
        enddo
      endif
      !!$ U is the knot vector
      !!$ p is the polynomial order
      !!$ n is the index of the last control point
      !!$ nelem you get from running CountSpans
      !!$ M is the dense matrix
c-> DEBUG
      if(idebug.eq.1)then
        MFULL=0
        call Form1DMassMatrixFULL(U,p,n,nelem,MFULL) 
        if(iprint.eq.1)then
          write(*,*)'MFULL='
          do i=1,n+1
            write(*,*)i,'row=',MFULL(i,1:n+1)
          enddo
          call pause
        endif
      endif
c<- DEBUG

c
c  ...generate the RHS vectors
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)
      
      F=0
      call Form3DRHS
     .   (U,p,n,nelem,U,p,n,nelem,U,p,n,nelem,
     .    ibegx,iendx,MYRANKX,NRPROCX,
     .    ibegy,iendy,MYRANKY,NRPROCY,
     .    ibegz,iendz,MYRANKZ,NRPROCZ,F)
      if(iprint.eq.1)then
        write(*,*)PRINTRANK,'F'
        NB=(iendy-ibegy+1)*(iendz-ibegz+1)
        do i=1,iendx-ibegx+1
          write(*,*)PRINTRANK,F(i,1:NB)
        enddo
      endif
c-> DEBUG
      if(idebug.eq.1)then
        call Form3DRHSFULL
     .   (U,p,n,nelem,U,p,n,nelem,U,p,n,nelem,FFULL)
        if(iprint.eq.1)then
          write(*,*)'FFULL=',FFULL
          call pause
        endif
      endif
c<- DEBUG
cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc
c  ...solve the first problem
c
c      if(iprint.eq.1)then
	write(*,*)PRINTRANK,'SOLVE THE FIRST PROBLEM'
        call pause
c      endif
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)

*  TRANS   (global input) CHARACTER
*          = 'N':  Solve with A(1:N, JA:JA+N-1);
      TRANS = 'N'
*  N       (global input) INTEGER
*          The number of rows and columns to be operated on, i.e. the
*          order of the distributed submatrix A(1:N, JA:JA+N-1). N >= 0.
      Nhere=n+1
*  BWL     (global input) INTEGER
*          Number of subdiagonals. 0 <= BWL <= N-1
      BWL = KL
*  BWU     (global input) INTEGER
*          Number of superdiagonals. 0 <= BWU <= N-1
      BWU = KU
*  NRHS    (global input) INTEGER
*          The number of right hand sides, i.e., the number of columns
*          of the distributed submatrix B(IB:IB+N-1, 1:NRHS).
*          NRHS >= 0.
      NRHS = NB
*  A       (local input/local output) DOUBLE PRECISION pointer into
*          local memory to an array with first dimension
*          LLD_A >=(bwl+bwu+1) (stored in DESCA).
*          On entry, this array contains the local pieces of the
*          N-by-N unsymmetric banded distributed Cholesky factor L or
*          L^T A(1:N, JA:JA+N-1).
*          This local portion is stored in the packed banded format
*            used in LAPACK. Please see the Notes below and the
*            ScaLAPACK manual for more detail on the format of
*            distributed matrices.
*
*  JA      (global input) INTEGER
*          The index in the global array A that points to the start of
*          the matrix to be operated on (which may be either all of A
*          or a submatrix of A).
      JA=0
*  DESCA   (global and local input) INTEGER array of dimension DLEN.
*          if 1D type (DTYPE_A=501), DLEN >= 7;
*          if 2D type (DTYPE_A=1), DLEN >= 9 .
*          The array descriptor for the distributed matrix A.
*          Contains information of mapping of A to memory. Please
*          see NOTES below for full description and options.
*
*  B       (local input/local output) DOUBLE PRECISION pointer into
*          local memory to an array of local lead dimension lld_b>=NB.
*          On entry, this array contains the
*          the local pieces of the right hand sides
*          B(IB:IB+N-1, 1:NRHS).
*          On exit, this contains the local piece of the solutions
*          distributed matrix X.
*
*  IB      (global input) INTEGER
*          The row index in the global array B that points to the first
*          row of the matrix to be operated on (which may be either
*          all of B or a submatrix of B).
      IB=0
*  DESCB   (global and local input) INTEGER array of dimension DLEN.
*          if 1D type (DTYPE_B=502), DLEN >=7;
*          if 2D type (DTYPE_B=1), DLEN >= 9.
*          The array descriptor for the distributed matrix B.
*          Contains information of mapping of B to memory. Please
*          see NOTES below for full description and options.
*
*  AF      (local output) DOUBLE PRECISION array, dimension LAF.
*          Auxiliary Fillin Space.
*          Fillin is created during the factorization routine
*          PDDBTRF and this is stored in AF. If a linear system
*          is to be solved using PDDBTRS after the factorization
*          routine, AF *must not be altered* after the factorization.
*
*  LAF     (local input) INTEGER
*          Size of user-input Auxiliary Fillin space AF. Must be >=
*          NB*(bwl+bwu)+6*max(bwl,bwu)*max(bwl,bwu)
*          If LAF is not large enough, an error code will be returned
*          and the minimum acceptable size will be returned in AF( 1 )
*
*  WORK    (local workspace/local output)
*          DOUBLE PRECISION temporary workspace. This space may
*          be overwritten in between calls to routines. WORK must be
*          the size given in LWORK.
*          On exit, WORK( 1 ) contains the minimal LWORK.
*
*  LWORK   (local input or global input) INTEGER
*          Size of user-input workspace WORK.
*          If LWORK is too small, the minimal acceptable size will be
*          returned in WORK(1) and an error code is returned. LWORK>=
*          (max(bwl,bwu)*NRHS)
c  ...size of mthe
      LAF=NB*(KL+KU)+6*max(KL,KU)*max(KL,KU)
      LWORK=max(KL,KU)*NRHS
      allocate(AF(LAF))
      allocate(WORK(LWORK))
*  INFO    (global output) INTEGER
*          = 0:  successful exit
*          < 0:  If the i-th argument is an array and the j-entry had
*                an illegal value, then INFO = -(i*100+j), if the i-th
*                argument is a scalar and had an illegal value, then
*                INFO = -i.


      if(iprint.eq.1)then
        write(*,*)'CALL PDDBTRS'
        write(*,*)'N=',N
        write(*,*)'BWL=',BWL
        write(*,*)'BKU=',BWU
        write(*,*)'NRHS=',NRHS
        write(*,*)'AB='
        do i=1,KL+KU+1
          write(*,*)i,'row=',M(i,1:iendx-ibegx)
        enddo
        write(*,*)'JA=',JA
        write(*,*)'DESCA=',DESCA
        write(*,*)'B='
        do i=1,iendx-ibegx
          write(*,*)i,'row=',F(i,1:(iendy-ibegy+1)*(iendz-ibegz+1))
        enddo
        write(*,*)'IB=',IB
        write(*,*)'DESCA=',DESCB
      endif
c-> parallel pddbtrs
c TO DO: prepare DESCA, DESCB, AF,LAF,WORK,LWORK
c      call DGBSV(n+1,KL,KU,(n+1)*(n+1),M,2*KL+KU+1,IPIV,F,n+1,iret)
      call PDDBTRS(TRANS,Nhere,BWL,BWU,NRHS,M,JA,DESCA,F,IB,DESCB,
     .   AF,LAF,WORK,LWORK,iret)
c<- parallel
c      if(iprint.eq.1)then
        write(*,*)'iret=',iret
        write(*,*)'Solution='
        do i=1,iendx-ibegx
          write(*,*)i,'row=',F(i,1:(iendy-ibegy+1)*(iendz-ibegz+1))
        enddo
c        call pause
c      endif
c  ...reorder right hand sides
      
      call mpi_barrier(MPI_COMM_WORLD,ierr)
      stop

      call ReorderRHSForY
     .   (U,p,n,nelem,U,p,n,nelem,U,p,n,nelem, 
     .    ibegx,iendx,MYRANKX,NRPROCX,
     .    ibegy,iendy,MYRANKY,NRPROCY,
     .    ibegz,iendz,MYRANKZ,NRPROCZ,F,F2)

c  ...solve the second problem
      if(iprint.eq.1)then
	write(*,*)PRINTRANK,'SOLVE THE SECOND PROBLEM'
        call pause
      endif
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)

      M=0
      call Form1DMassMatrix(KL,KU,U,p,n,nelem, 
     .   ibegy,iendy,MYRANKY,NRPROCY,M) 
c
      DESCA(1)=501
      DESCA(2)=COMMY !BLACS context handle
      DESCA(3)=(n+1) !number of rows in global matrix
      DESCA(4)=(n+1)/nrprocy !blocking factor used to distribute columns of the array
      DESCA(5)=processorsy(1) !first processor where the matrix is distributed
      DESCA(6)=iendy-ibegy!leading dimension of the distributed array
      DESCA(7)=0 !unused
      DESCB(1)=501
      DESCB(2)=COMMY !BLACS context handle
      DESCB(3)=(n+1)*(n+1)*(n+1) !number of rows in global matrix
      DESCB(4)=NB !blocking factor used to distribute columns of the array
      DESCB(5)=processorsy(1) !first processor where the matrix is distributed
      DESCB(6)=NB !leading dimension of the distributed array
      DESCB(7)=0  !unused
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)
c      
      if(iprint.eq.1)then
        write(*,*)'CALL PDDBTRS'
        write(*,*)'N=',n+1
        write(*,*)'KL=',KL
        write(*,*)'KU=',KU
        write(*,*)'AB='
        do i=1,KL+KU+1
          write(*,*)i,'row=',M(i,1:iendy-ibegy)
        enddo
        write(*,*)'JA=',0
        write(*,*)'DESCA=',DESCA
        write(*,*)'B='
        do i=1,iendy-ibegy
          write(*,*)i,'row=',F2(i,1:(iendx-ibegx+1)*(iendz-ibegz+1))
        enddo
        write(*,*)'JB=',0
      endif
c-> parallel pddbtrs
c TO DO: prepare DESCA, DESCB, AF,LAF,WORK,LWORK
c      call DGBSV(n+1,KL,KU,(n+1)*(n+1),M,2*KL+KU+1,IPIV,F,n+1,iret)
      call PDDBTRS('N',n+1,KL,KU,NRHS,M,0,DESCA,F,0,DESCB,
     .   AF,LAF,WORK,LWORK,iret)
c<- parallel
      if(iprint.eq.1)then
        write(*,*)'iret=',iret
        write(*,*)'Solution='
        do i=1,iendy-ibegy+1
          write(*,*)i,'row=',F2(i,1:(iendx-ibegx+1)*(iendz-ibegz+1))
        enddo
        call pause
      endif
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)
c
c  ...reorder right hand sides
      call ReorderRHSForZ
     .   (U,p,n,nelem,U,p,n,nelem,U,p,n,nelem, 
     .    ibegx,iendx,MYRANKX,NRPROCX,
     .    ibegy,iendy,MYRANKY,NRPROCY,
     .    ibegz,iendz,MYRANKZ,NRPROCZ,F2,F3)
c  ...solve the third problem
c
      if(iprint.eq.1)then
	write(*,*)PRINTRANK,'SOLVE THE THIRD PROBLEM'
        call pause
      endif
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)
c
      M=0
      call Form1DMassMatrix(KL,KU,U,p,n,nelem,
     .   ibegz,iendz,MYRANKZ,NRPROCZ,M)  
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)
c
      DESCA(1)=501
      DESCA(2)=COMMZ !BLACS context handle
      DESCA(3)=(n+1) !number of rows in global matrix
      DESCA(4)=(n+1)/nrprocz !blocking factor used to distribute columns of the array
      DESCA(5)=processorsz(1) !first processor where the matrix is distributed
      DESCA(6)=iendz-ibegz!leading dimension of the distributed array
      DESCA(7)=0 !unused
      DESCB(1)=501
      DESCB(2)=COMMZ !BLACS context handle
      DESCB(3)=(n+1)*(n+1)*(n+1) !number of rows in global matrix
      DESCB(4)=NB !blocking factor used to distribute columns of the array
      DESCB(5)=processorsz(1) !first processor where the matrix is distributed
      DESCB(6)=NB !leading dimension of the distributed array
      DESCB(7)=0  !unused
c
      if(iprint.eq.1)then
        write(*,*)'CALL PDDBTRS'
        write(*,*)'N=',n+1
        write(*,*)'KL=',KL
        write(*,*)'KU=',KU
        write(*,*)'AB='
        do i=1,KL+KU+1
          write(*,*)i,'row=',M(i,1:iendz-ibegz)
        enddo
        write(*,*)'JA=',0
        write(*,*)'DESCA=',DESCA
        write(*,*)'B='
        do i=1,iendz-ibegz
          write(*,*)i,'row=',F2(i,1:(iendx-ibegx+1)*(iendy-ibegy+1))
        enddo
        write(*,*)'JB=',0
      endif
c-> parallel pddbtrs
c TO DO: prepare DESCA, DESCB, AF,LAF,WORK,LWORK
c      call DGBSV(n+1,KL,KU,(n+1)*(n+1),M,2*KL+KU+1,IPIV,F,n+1,iret)
      call PDDBTRS('N',n+1,KL,KU,NRHS,M,0,DESCA,F,0,DESCB,
     .   AF,LAF,WORK,LWORK,iret)
c<- parallel
      if(iprint.eq.1)then
        write(*,*)'iret=',iret
        write(*,*)'Solution='
        do i=1,iendz-ibegz+1
          write(*,*)i,'row=',F3(i,1:(iendx-ibegx+1)*(iendz-ibegz+1))
        enddo
        call pause
      endif
c
      stop
c
      deallocate(U)
      deallocate(M)
      deallocate(F)
      deallocate(F2)
      deallocate(F3)
c-> DEBUG
      if(idebug.eq.1)then
        deallocate(MFULL)
        deallocate(FFULL)
      endif
c<- DEBUG
      deallocate(DESCA)
      deallocate(DESCB)
      deallocate(AF)
      deallocate(WORK)
c
      call mpi_barrier(MPI_COMM_WORLD,ierr)
      call mpi_finalize(ierr)
      write(*,*)PRINTRANK,"Exiting..."
c
      end




